{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7cb0ab",
   "metadata": {},
   "source": [
    "## PyTorch로 경사하강법(Gradient Descent) 구현\n",
    "\n",
    "기본 개념은 함수의 기울기(경사)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜서 극값에 이를 때까지 반복시키는 것입니다.\n",
    "\n",
    "**비용 함수 (Cost Function 혹은 Loss Function)를 최소화**하기 위해 반복해서 파라미터를 업데이트 해 나가는 방식입니다.\n",
    "\n",
    "경사하강법에 대한 상세한 설명은 아래 링크를 참고해 주시기 바랍니다.\n",
    "\n",
    "- [경사하강법 구현](https://teddylee777.github.io/scikit-learn/gradient-descent)\n",
    "- [경사하강법 기본 개념(YouTube)](https://www.youtube.com/watch?v=GEdLNvPIbiM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import \n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b37d23",
   "metadata": {},
   "source": [
    "## 샘플 데이터셋 생성\n",
    "\n",
    "- `y = 0.3x + 0.5`의 선형회귀 식을 추종하는 샘플 데이터셋을 생성합니다.\n",
    "- 경사하강법 알고리즘으로 `w=0.3`, `b=0.5`를 추종하는 결과를 도출하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8305e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear(w=0.5, b=0.8, size=50, noise=1.0):\n",
    "    x = np.random.rand(size)\n",
    "    y = w * x + b\n",
    "    noise = np.random.uniform(-abs(noise), abs(noise), size=y.shape)\n",
    "    yy = y + noise\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(x, y, color='r', label=f'y = {w}x + {b}', linestyle=':', alpha=0.3)\n",
    "    plt.scatter(x, yy, color='black', label='data', marker='.')\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.show()\n",
    "    print(f'w: {w}, b: {b}')\n",
    "    return x, yy\n",
    "\n",
    "x, y = make_linear(w=0.3, b=0.5, size=100, noise=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebd360",
   "metadata": {},
   "source": [
    "샘플 데이터셋인 `x`와 `y`를 `torch.as_tensor()`로 텐서(Tensor)로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터셋을 텐서(tensor)로 변환\n",
    "x = torch.as_tensor(x)\n",
    "y = torch.as_tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc185d",
   "metadata": {},
   "source": [
    "랜덤한 `w`, `b`를 생성합니다. `torch.rand(1)`은 `torch.Size([1])`을 가지는 normal 분포의 랜덤 텐서를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d09648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 한 값으로 w, b를 초기화 합니다.\n",
    "w = torch.rand(1)\n",
    "b = torch.rand(1)\n",
    "\n",
    "print(w.shape, b.shape)\n",
    "\n",
    "# requires_grad = True로 설정된 텐서에 대해서만 미분을 계산합니다.\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fba18f",
   "metadata": {},
   "source": [
    "다음은 가설함수(Hypothesis Function), 여기서는 Affine Function을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc20906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Function 정의\n",
    "y_hat = w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74b444",
   "metadata": {},
   "source": [
    "`y_hat`과 `y`의 손실(Loss)을 계산합니다. 여기서 손실함수는 **Mean Squared Error** 함수를 사용합니다.\n",
    "\n",
    "$\\Large Loss = \\sum_{i=1}^{N}(\\hat{y}_i-y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의\n",
    "loss = ((y_hat - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bc852",
   "metadata": {},
   "source": [
    "`loss.backward()` 호출시 미분 가능한 텐서(Tensor)에 대하여 미분을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b637996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 계산 (Back Propagation)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de9d23",
   "metadata": {},
   "source": [
    "`w`와 `b`의 미분 값을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계산된 미분 값 확인\n",
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe216f",
   "metadata": {},
   "source": [
    "## 경사하강법 구현\n",
    "\n",
    "- 최대 500번의 iteration(epoch) 동안 반복하여 w, b의 미분을 업데이트 하면서, 최소의 손실(loss)에 도달하는 `w`, `b`를 산출합니다.\n",
    "- `learning_rate`는 임의의 값으로 초기화 하였으며, `0.1`로 설정하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f8608",
   "metadata": {},
   "source": [
    "하이퍼파라미터(hyper-parameter) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 반복 횟수 정의\n",
    "num_epoch = 500\n",
    "\n",
    "# 학습율 (learning_rate)\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, w, b 기록하기 위한 list 정의\n",
    "losses = []\n",
    "ws = []\n",
    "bs = []\n",
    "\n",
    "# random 한 값으로 w, b를 초기화 합니다.\n",
    "w = torch.rand(1)\n",
    "b = torch.rand(1)\n",
    "\n",
    "# 미분 값을 구하기 위하여 requires_grad는 True로 설정\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    # Affine Function\n",
    "    y_hat = x * w + b\n",
    "\n",
    "    # 손실(loss) 계산\n",
    "    loss = ((y_hat - y)**2).mean()\n",
    "    \n",
    "    # 손실이 0.00005보다 작으면 break 합니다.\n",
    "    if loss < 0.00005:\n",
    "        break\n",
    "\n",
    "    # w, b의 미분 값인 grad 확인시 다음 미분 계산 값은 None이 return 됩니다.\n",
    "    # 이러한 현상을 방지하기 위하여 retain_grad()를 loss.backward() 이전에 호출해 줍니다.\n",
    "    w.retain_grad()\n",
    "    b.retain_grad()\n",
    "    \n",
    "    # 미분 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사하강법 계산 및 적용\n",
    "    # w에 learning_rate * (그라디언트 w) 를 차감합니다.\n",
    "    w = w - learning_rate * w.grad\n",
    "    # b에 learning_rate * (그라디언트 b) 를 차감합니다.\n",
    "    b = b - learning_rate * b.grad\n",
    "    \n",
    "    # 계산된 loss, w, b를 저장합니다.\n",
    "    losses.append(loss.item())\n",
    "    ws.append(w.item())\n",
    "    bs.append(b.item())\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"{0:03d} w = {1:.5f}, b = {2:.5f} loss = {3:.5f}\".format(epoch, w.item(), b.item(), loss.item()))\n",
    "    \n",
    "print(\"----\" * 15)\n",
    "print(\"{0:03d} w = {1:.1f}, b = {2:.1f} loss = {3:.5f}\".format(epoch, w.item(), b.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49ebb7",
   "metadata": {},
   "source": [
    "## 결과 시각화\n",
    "\n",
    "- `loss`는 epoch이 늘어남에 따라 감소합니다.\n",
    "-  epoch 초기에는 급격히 감소하다가, 점차 완만하게 감소함을 확인할 수 있는데, 이는 초기에는 큰 미분 값이 업데이트 되지만, 점차 계산된 미분 값이 작아지게되고 결국 업데이트가 작게 일어나면서 손실은 완만하게 감소하였습니다.\n",
    "- `w`, `b`도 초기값은 `0.3`, `0.5`와 다소 먼 값이 설정되었지만, 점차 정답을 찾아가게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 loss 에 대한 변화량 시각화\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(losses, c='darkviolet', linestyle=':')\n",
    "\n",
    "plt.title('Losses over epoches', fontsize=15)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# w, b에 대한 변화량 시각화\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(14, 6)\n",
    "\n",
    "axes[0].plot(ws, c='tomato', linestyle=':', label='chages')\n",
    "axes[0].hlines(y=0.3, xmin=0, xmax=len(ws), color='r', label='true')\n",
    "axes[0].set_ylim(0, 0.7)\n",
    "axes[0].set_title('\"w\" changes over epoches', fontsize=15)\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Error')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(bs, c='dodgerblue', linestyle=':', label='chages')\n",
    "axes[1].hlines(y=0.5, xmin=0, xmax=len(ws), color='dodgerblue', label='true')\n",
    "axes[1].set_ylim(0.2, 0.9)\n",
    "axes[1].set_title('\"b\" changes over epoches', fontsize=15)\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
