{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed64781",
   "metadata": {},
   "source": [
    "## GoogLeNet의 Inception Module 구현\n",
    "\n",
    "Going Deeper with Convolutions(2015) Inception 모듈에 대한 내용입니다. 해당 논문에서는 Inception Module이라는 새로운 neural network architecture 를 공개하였습니다. 논문의 제목과 같이 Going Deeper 즉 더욱 깊은 신경망 모델을 dimension reduction이 적용된 Inception Module로 가능케 하였는데, 이때 1x1 Convolution을 적극 활용하였습니다.\n",
    "\n",
    "이때 활용한 1x1 Convolution이 어떤 역할을 하였는지 살펴보도록 하겠습니다.\n",
    "\n",
    "- 논문 링크 [**(링크)**](https://arxiv.org/pdf/1409.4842v1.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471b5ef",
   "metadata": {},
   "source": [
    "먼저 아래 그림은 논문에서 나온 초기(naive) 버전의 Inception Module 입니다. 궁극적으로 아래의 Inception Module을 활용하여 이미지의 지역적 특성 추출을 효과적으로 수행할 수 있도록 구조를 설계하였습니다.\n",
    "\n",
    "하나의 이미지 영역에 대하여 1x1, 3x3, 5x5 컨볼루션과 3x3 Maxpooling 층으로 특성 추출을 한 뒤 필터에 대한 concatenation을 수행합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99f57d",
   "metadata": {},
   "source": [
    "> GoogLeNet Inception Module naive version (Version 1)\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*wverTCLSTVNDpyRhlGVwyw.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82d19f",
   "metadata": {},
   "source": [
    "위의 그림대로 Inception Module을 구현한 코드는 아래와 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "\n",
    "\n",
    "class BaseConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BaseConv2D, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class InceptionModuleV1(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3, out_5x5, pool):\n",
    "        super(InceptionModuleV1, self).__init__()\n",
    "        self.conv1x1 = BaseConv2D(in_channels, out_1x1, kernel_size=1)\n",
    "        self.conv3x3 = BaseConv2D(\n",
    "            in_channels, out_3x3, kernel_size=3, padding='same')\n",
    "        self.conv5x5 = BaseConv2D(\n",
    "            in_channels, out_5x5, kernel_size=5, padding='same')\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BaseConv2D(in_channels, pool, kernel_size=1, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.pool(x)\n",
    "        return torch.cat([x1, x2, x3, x4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b228618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 매개변수: in_channels, 1x1, 3x3, 5x5 컨볼루션 output 필터 사이즈, Maxpooling output 필터 사이즈\n",
    "inception_module_V1 = InceptionModuleV1(192, 64, 128, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchsummary 로 192 x 28 x 28 입력을 주었을 때의 파라미터 추정\n",
    "torchsummary.summary(inception_module_V1,\n",
    "                     input_size=(192, 28, 28), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a3c9a",
   "metadata": {},
   "source": [
    "28 x 28 크기의 사진 192장을 입력으로 주었을 때 학습 파라미터는 **383k** 정도가 나왔습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6b3a0",
   "metadata": {},
   "source": [
    "## 1x1 컨볼루션을 활용한 dimension reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7eea26",
   "metadata": {},
   "source": [
    "이제 1x1 Convolution을 적용한 Inception Module을 알아볼 차례입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa465dd9",
   "metadata": {},
   "source": [
    "> Inception Module with dimension reductions (Version 2)\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*SdbkFi2JB-Tjri7LVOMkWA.webp)\n",
    "\n",
    "출처: https://valentinaalto.medium.com/understanding-the-inception-module-in-googlenet-2e1b7c406106\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8428b91",
   "metadata": {},
   "source": [
    "위의 그림은 Inception Module에 dimension reductions를 적용한 구조도입니다.\n",
    "\n",
    "3x3, 5x5 컨볼루션을 통과하기 전에 **1x1 컨볼루션이 추가**된 것을 확인할 수 있습니다. 또한, 3x3 Maxpooling을 통과한 후 1x1 컨볼루션을 통과하는 것도 확인할 수 있습니다.\n",
    "\n",
    "자, 그럼 1x1 컨볼루션을 추가한 의미는 무엇이며, 궁극적으로 어떻게 GoogLeNet의 모델의 학습 성과를 올릴 수 있었는지 살펴 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de2da0",
   "metadata": {},
   "source": [
    "![](https://blog.kakaocdn.net/dn/KqguG/btqAyyU6Dlp/4k2pZ9IiJeZT3lB0KBotgK/img.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260972a",
   "metadata": {},
   "source": [
    "위의 그림에서\n",
    "\n",
    "1. 첫 번째는 1x1 컨볼루션을 활용하지 않고 28x28 사진(특성맵) 64장을 만들어내는 결과이고\n",
    "2. 두 번째는 1x1 컨볼루션을 활용하여 dimension reduction을 수행 후 28x28 사진(특성맵) 64장을 만들어내는 결과입니다.\n",
    "\n",
    "(컨볼루션 연산시 zero-padding이 추가되어 Feature Map의 이미지는 줄어들지 않았음을 참고해 주세요!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae728b6",
   "metadata": {},
   "source": [
    "1. 첫 번째는 160M의 학습 파라미터가 사용되고,\n",
    "2. 두 번째는 첫 번째 대비 학습 파라미터의 개수가 44.8M로 훨씬 더 적습니다.이유는 1x1 컨볼루션 32가 먼저 필터의 개수를 줄이고, 이후에 5x5컨볼루션 64가 수행되면서 연상량을 획기적으로 절약할 수 있었지만, 동일한 28x28x64의 아웃풋을 낼 수 있습니다.\n",
    "\n",
    "이렇듯 **1x1 컨볼루션에 필터의 갯수를 줄여 연산량을 획기적으로 감소**할 수 있습니다. 이것이 바로 **dimension reductions의 핵심**입니다. 이렇게 1x1 컨볼루션에서 필터 개수를 줄인 뒤 다시 키우는 구조를 **BottleNeck** 이라고 부르기도 합니다.\n",
    "\n",
    "결국, 이렇게 절약한 연산량 덕분에 더 깊은 모델을 생성해낼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432b2d5",
   "metadata": {},
   "source": [
    "## Inception Module with dimension reductions 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModuleV2(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3_reduce, out_3x3, out_5x5_reduce, out_5x5, pool):\n",
    "        super(InceptionModuleV2, self).__init__()\n",
    "        self.conv1x1 = BaseConv2D(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            # 1x1 Convolution\n",
    "            BaseConv2D(in_channels, out_3x3_reduce, kernel_size=1),\n",
    "            # 3x3 Convolution\n",
    "            BaseConv2D(out_3x3_reduce, out_3x3, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.conv5x5 = nn.Sequential(\n",
    "            # 1x1 Convolution\n",
    "            BaseConv2D(in_channels, out_5x5_reduce, kernel_size=1),\n",
    "            # 5x5 Convolution\n",
    "            BaseConv2D(out_5x5_reduce, out_5x5, kernel_size=5, padding='same')\n",
    "        )\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            # Maxpooling\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            # 1x1 Convolution\n",
    "            BaseConv2D(in_channels, pool, kernel_size=1, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.pool(x)\n",
    "        return torch.cat([x1, x2, x3, x4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 매개변수: in_channels, 1x1, 3x3 reduction, 3x3, 5x5 reduction, 5x5, Maxpool reduction\n",
    "inception_module_V2 = InceptionModuleV2(192, 64, 96, 128, 16, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90182879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchsummary 로 192 x 28 x 28 입력을 주었을 때의 파라미터 추정\n",
    "torchsummary.summary(inception_module_V2,\n",
    "                     input_size=(192, 28, 28), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887713d",
   "metadata": {},
   "source": [
    "- **초기 버전**: 28 x 28 크기의 사진 192장을 입력으로 주었을 때 학습 파라미터는 **383k** 정도가 나왔습니다.\n",
    "- **V2(dimension reductions 적용)**: 28 x 28 크기의 사진 192장을 입력으로 주었을 때 학습 파라미터는 **163k** 정도가 나왔습니다.\n",
    "\n",
    "확실히 파라미터 개수가 큰 차이를 보여줍니다. 아래는 Inception Module V1, V2를 통과한 결과의 Shape 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더미 데이터 생성 (192x28x28)\n",
    "dummy_input = torch.randn(size=(1, 192, 28, 28))\n",
    "# V1 통과\n",
    "y1 = inception_module_V1(dummy_input)\n",
    "# V2 통과\n",
    "y2 = inception_module_V2(dummy_input)\n",
    "# Shape 출력\n",
    "print(y1.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9223aa",
   "metadata": {},
   "source": [
    "결과는 V1, V2 모두 동일한 Shape가 출력됨을 확인할 수 있습니다. 즉, **결과물의 Shape는 동일**하나 파라미터 차이가 약 **220k** 정도 납니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
