{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed64781",
   "metadata": {},
   "source": [
    "## GoogLeNet 구현\n",
    "\n",
    "Going Deeper with Convolutions(2015) 의 PyTorch 구현 입니다. 논문에 대한 세부 인사이트는 생략하며, 오직 코드 구현만 다룹니다.\n",
    "\n",
    "- 논문 링크 [**(링크)**](https://arxiv.org/pdf/1409.4842v1.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a75c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter 설정\n",
    "\n",
    "IMAGE_SIZE = 224  # AlexNet의 이미지 입력 크기는 (3, 227, 227) 입니다.\n",
    "NUM_EPOCHS = 10\n",
    "LR = 0.0001  # Learning Rate\n",
    "\n",
    "MODEL_NAME = 'GoogLeNet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362052a",
   "metadata": {},
   "source": [
    "## 학습에 활용할 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fba4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import sample_datasets as sd\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # 개와 고양이 사진 파일의 크기가 다르므로, Resize로 맞춰줍니다.\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((IMAGE_SIZE, IMAGE_SIZE)),      # 중앙 Crop\n",
    "    transforms.RandomHorizontalFlip(0.5),   # 50% 확률로 Horizontal Flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225)),  # 이미지 정규화\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    # 개와 고양이 사진 파일의 크기가 다르므로, Resize로 맞춰줍니다.\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225)),  # 이미지 정규화\n",
    "])\n",
    "\n",
    "train_loader, test_loader = sd.cats_and_dogs(train_transform, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 배치를 추출합니다.\n",
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef26410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 shape을 확인합니다. 224 X 224 RGB 이미지 임을 확인합니다.\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d88005",
   "metadata": {},
   "source": [
    "CUDA 설정이 되어 있다면 `cuda`를! 그렇지 않다면 `cpu`로 학습합니다.\n",
    "\n",
    "(제 PC에는 GPU가 2대 있어서 `cuda:0`로 GPU 장비의 index를 지정해 주었습니다. 만약 다른 장비를 사용하고 싶다면 `cuda:1` 이런식으로 지정해 주면 됩니다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# device 설정 (cuda:0 혹은 cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99f57d",
   "metadata": {},
   "source": [
    "> GoogLeNet Inception Module (Version 1)\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*wverTCLSTVNDpyRhlGVwyw.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e1b81",
   "metadata": {},
   "source": [
    "![](https://devlee247.com/assets/img/blog/GoogleNet1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa465dd9",
   "metadata": {},
   "source": [
    "> Inception Module with Bottle Neck (Version 2)\n",
    "\n",
    "![](https://miro.medium.com/max/720/1*SdbkFi2JB-Tjri7LVOMkWA.webp)\n",
    "\n",
    "출처: https://valentinaalto.medium.com/understanding-the-inception-module-in-googlenet-2e1b7c406106\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BaseConv2D, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3_reduce, out_3x3, out_5x5_reduce, out_5x5, pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.conv1x1 = BaseConv2D(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            BaseConv2D(in_channels, out_3x3_reduce, kernel_size=1),\n",
    "            BaseConv2D(out_3x3_reduce, out_3x3, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.conv5x5 = nn.Sequential(\n",
    "            BaseConv2D(in_channels, out_5x5_reduce, kernel_size=1),\n",
    "            BaseConv2D(out_5x5_reduce, out_5x5, kernel_size=5, padding='same')\n",
    "        )\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BaseConv2D(in_channels, pool, kernel_size=1, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.pool(x)\n",
    "        return torch.cat([x1, x2, x3, x4], 1)\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, out_3x3_reduce, out_3x3, out_5x5_reduce, out_5x5, pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.conv1x1 = BaseConv2D(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            BaseConv2D(in_channels, out_3x3_reduce, kernel_size=1),\n",
    "            BaseConv2D(out_3x3_reduce, out_3x3, kernel_size=3, padding='same')\n",
    "        )\n",
    "        self.conv5x5 = nn.Sequential(\n",
    "            BaseConv2D(in_channels, out_5x5_reduce, kernel_size=1),\n",
    "            BaseConv2D(out_5x5_reduce, out_5x5, kernel_size=5, padding='same')\n",
    "        )\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BaseConv2D(in_channels, pool, kernel_size=1, padding='same')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1(x)\n",
    "        x2 = self.conv3x3(x)\n",
    "        x3 = self.conv5x5(x)\n",
    "        x4 = self.pool(x)\n",
    "        return torch.cat([x1, x2, x3, x4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = InceptionModule(3, 64, 96, 128, 16, 32, 32)\n",
    "conv2d = BaseConv2D(3, 64, kernel_size=5, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = BaseConv2D(12, 24, kernel_size=1, padding=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90182879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32052d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(conv1, input_size=(12, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d74661",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(inception, input_size=(3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca82483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd78de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = inception(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99001f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(y, params=dict(inception.named_parameters()), show_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # Image input_size=(3, 227, 227)\n",
    "        self.layers = nn.Sequential(\n",
    "            # input_size=(96, 55, 55)\n",
    "            nn.Conv2d(in_channels=3, out_channels=96,\n",
    "                      kernel_size=(11, 11), stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            # input_size=(96, 27, 27)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # input_size=(256, 27, 27)\n",
    "            nn.Conv2d(in_channels=96, out_channels=256,\n",
    "                      kernel_size=(5, 5), stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            # input_size=(256, 13, 13)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # input_size=(384, 13, 13)\n",
    "            nn.Conv2d(in_channels=256, out_channels=384,\n",
    "                      kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # input_size=(384, 13, 13)\n",
    "            nn.Conv2d(in_channels=384, out_channels=384,\n",
    "                      kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # input_size=(256, 13, 13)\n",
    "            nn.Conv2d(in_channels=384, out_channels=256,\n",
    "                      kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # input_size=(256, 6, 6)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=256*6*6, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4096, out_features=1000),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(-1, 256*6*6)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da319911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "\n",
    "model = AlexNet()\n",
    "model.to(device)\n",
    "\n",
    "# AlexNet의 Image 입력 사이즈는 (3, 227, 227) 입니다.\n",
    "torchsummary.summary(model, input_size=(3, 227, 227), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저를 정의합니다. 옵티마이저에는 model.parameters()를 지정해야 합니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# 손실함수(loss function)을 지정합니다. Multi-Class Classification 이기 때문에 CrossEntropy 손실을 지정하였습니다.\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82586144",
   "metadata": {},
   "source": [
    "## 훈련(Train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "    model.train()\n",
    "\n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_size = 0\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "\n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(data_loader)\n",
    "\n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for batch_idx, (img, lbl) in enumerate(prograss_bar, start=1):\n",
    "        # image, label 데이터를 device에 올립니다.\n",
    "        img, lbl = img.to(device), lbl.to(device)\n",
    "\n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "        output = model(img)\n",
    "\n",
    "        # 손실함수에 output, label 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, lbl)\n",
    "\n",
    "        # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "        loss.backward()\n",
    "\n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n",
    "\n",
    "        # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "        # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "        _, pred = output.max(dim=1)\n",
    "\n",
    "        # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "        # 합계는 corr 변수에 누적합니다.\n",
    "        corr += pred.eq(lbl).sum().item()\n",
    "\n",
    "        # loss 값은 1개 배치의 평균 손실(loss) 입니다. img.size(0)은 배치사이즈(batch size) 입니다.\n",
    "        # loss 와 img.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "        # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "        running_size += img.size(0)\n",
    "        prograss_bar.set_description(\n",
    "            f'[Training] loss: {running_loss / running_size:.4f}, accuracy: {corr / running_size:.4f}')\n",
    "\n",
    "    # 누적된 정답수를 전체 개수로 나누어 주면 정확도가 산출됩니다.\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "\n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader.dataset), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4275f37",
   "metadata": {},
   "source": [
    "## 평가(Evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다.\n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "\n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for img, lbl in data_loader:\n",
    "            # device에 데이터를 올립니다.\n",
    "            img, lbl = img.to(device), lbl.to(device)\n",
    "\n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(img)\n",
    "\n",
    "            # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "            # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "            _, pred = output.max(dim=1)\n",
    "\n",
    "            # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "            # 합계는 corr 변수에 누적합니다.\n",
    "            corr += torch.sum(pred.eq(lbl)).item()\n",
    "\n",
    "            # loss 값은 1개 배치의 평균 손실(loss) 입니다. img.size(0)은 배치사이즈(batch size) 입니다.\n",
    "            # loss 와 img.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "            # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "            running_loss += loss_fn(output, lbl).item() * img.size(0)\n",
    "\n",
    "        # validation 정확도를 계산합니다.\n",
    "        # 누적한 정답숫자를 전체 데이터셋의 숫자로 나누어 최종 accuracy를 산출합니다.\n",
    "        acc = corr / len(data_loader.dataset)\n",
    "\n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader.dataset), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac24e4f",
   "metadata": {},
   "source": [
    "## 모델 훈련(training) & 검증\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.inf\n",
    "\n",
    "# Epoch 별 훈련 및 검증을 수행합니다.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Model Training\n",
    "    # 훈련 손실과 정확도를 반환 받습니다.\n",
    "    train_loss, train_acc = model_train(\n",
    "        model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "    # 검증 손실과 검증 정확도를 반환 받습니다.\n",
    "    val_loss, val_acc = model_evaluate(model, test_loader, loss_fn, device)\n",
    "\n",
    "    # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "    if val_loss < min_loss:\n",
    "        print(\n",
    "            f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{MODEL_NAME}.pth')\n",
    "\n",
    "    # Epoch 별 결과를 출력합니다.\n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514ee1de",
   "metadata": {},
   "source": [
    "## 저장한 가중치 로드 후 검증 성능 측정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191eaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 저장한 가중치를 로드합니다.\n",
    "model.load_state_dict(torch.load(f\"{MODEL_NAME}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1db76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 검증 손실(validation loss)와 검증 정확도(validation accuracy)를 산출합니다.\n",
    "final_loss, final_acc = model_evaluate(model, test_loader, loss_fn, device)\n",
    "print(\n",
    "    f'evaluation loss: {final_loss:.5f}, evaluation accuracy: {final_acc:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
