{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d80b85",
   "metadata": {},
   "source": [
    "## 시드 고정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2300df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 시드설정\n",
    "SEED = 123\n",
    "\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dbbce",
   "metadata": {},
   "source": [
    "## 샘플 예제파일 다운로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json\"\n",
    "urllib.request.urlretrieve(url, \"sarcasm.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ce279",
   "metadata": {},
   "source": [
    "## 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e335ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"sarcasm.json\") as f:\n",
    "    datas = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(datas)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d2859",
   "metadata": {},
   "source": [
    "## 토큰화 (Word Tokenization)\n",
    "\n",
    "- get_tokenizer로 토크나이저 생성\n",
    "- `basic_english`, `spacy`, `revtok`, `subword` 등 지정이 가능하나, 몇몇 토크나이저는 추가 라이브러리 설치가 필요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67344a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext 설치\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# 토큰 생성\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecede1d",
   "metadata": {},
   "source": [
    "토큰화한 결과는 특수문자는 개별 토큰으로 처리, 모든 단어는 소문자로 처리됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hi, my name is Teddy!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, I would love to learn Python!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"안녕하세요? 한글 데이터에 대한 토큰 처리는 어떨까요??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8f542",
   "metadata": {},
   "source": [
    "## 단어사전 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae2553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def yield_tokens(sentences):\n",
    "    for text in sentences:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fe439",
   "metadata": {},
   "source": [
    "`build_vocab_from_iterator` 를 활용하여 단어 사전을 생성합니다.\n",
    "\n",
    "- `min_freq`: 최소 빈도의 토큰의 개수를 입력합니다.\n",
    "- `max_tokens`: 최대 빈도 토큰의 수를 한정합니다. 빈도수 기준으로 산정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71928f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df[\"headline\"].tolist()),  # 텍스트 Iterator\n",
    "    # 스페셜 토큰\n",
    "    specials=[\"<UNK>\"],\n",
    "    min_freq=2,  # 최소 빈도 토큰\n",
    "    max_tokens=1000,  # 최대 토큰 개수\n",
    ")\n",
    "\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어사전의 개수 출력\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05274e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string -> index\n",
    "stoi = vocab.get_stoi()\n",
    "# index -> string\n",
    "itos = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a18451",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi[\"trump\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"Hello, I am Teddy. Nice to meet you!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(tokenizer(sample_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc69ea",
   "metadata": {},
   "source": [
    "## Dataset 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df[\"headline\"],\n",
    "    df[\"is_sarcastic\"],\n",
    "    stratify=df[\"is_sarcastic\"],\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab9433",
   "metadata": {},
   "source": [
    "## Dataset 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bff8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        return self.vocab(self.tokenizer(text)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset 생성\n",
    "train_ds = CustomDataset(x_train, y_train, vocab=vocab, tokenizer=tokenizer)\n",
    "valid_ds = CustomDataset(x_test, y_test, vocab=vocab, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6973b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 데이터 추출\n",
    "text, label = next(iter(train_ds))\n",
    "len(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator 생성\n",
    "iterator = iter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133dd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next 로 순회하면서 1개씩 출력\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de142034",
   "metadata": {},
   "source": [
    "## DataLoader 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74296405",
   "metadata": {},
   "source": [
    "GPU 를 설정합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed101327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "if torch.backends.mps.is_built():\n",
    "    # mac os mps 지원 체크\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "else:\n",
    "    # cuda 사용 가능한지 체크\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca2422",
   "metadata": {},
   "source": [
    "DataLoader 를 생성합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_batch(batch, max_sequence_length):\n",
    "    label_list, text_list = [], []\n",
    "\n",
    "    for text, label in batch:\n",
    "        # 최대 문장길이를 넘어가는 단어는 제거합니다.\n",
    "        processed_text = torch.tensor(text[:max_sequence_length], dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    # padding을 주어 짧은 문장에 대한 길이를 맞춥니다.\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장에 최대 포함하는 단어의 개수를 지정합니다. (예시. 120 단어)\n",
    "MAX_SEQUENCE_LENGTH = 120\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_batch(x, MAX_SEQUENCE_LENGTH),\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: collate_batch(x, MAX_SEQUENCE_LENGTH),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba69383",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# x, y의 shape 확인\n",
    "# (batch_size, seq_length), (batch_size)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader의 it\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iterator)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3683d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83aa751",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fdb7b",
   "metadata": {},
   "source": [
    "## Embedding Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCAB = len(vocab)\n",
    "NUM_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "x.shape, y.shape\n",
    "# (batch_size, seq_length), (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795f862",
   "metadata": {},
   "source": [
    "`nn.Embedding()` 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding: (vocab_size, embedding_dim)\n",
    "EMBEDDING_DIM = 30  # Dimension을 30 차원으로 설정(hyper-parameter)\n",
    "embedding = nn.Embedding(len(vocab), EMBEDDING_DIM).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46254d79",
   "metadata": {},
   "source": [
    "`nn.Embedding()` 의 입출력 shape 에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aae29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x : (batch_size, seq_length)\n",
    "embedding_out = embedding(x)\n",
    "embedding_out.shape\n",
    "# embedding_out: (batch_size, seq_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e26d0",
   "metadata": {},
   "source": [
    "## LSTM Layer\n",
    "\n",
    "- 참고 링크: https://teddylee777.github.io/pytorch/pytorch-lstm/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca56291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=\"https://teddylee777.github.io/images/2023-03-05/lstm-shapes-01.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ff835",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30  # input_size: embedding_dim(임베딩 차원)\n",
    "HIDDEN_SIZE = 64  # hidden_size: 추출할 특성의 수(hyper-parameter)\n",
    "NUM_LAYERS = 1  # LSTM Stacking Layer 수\n",
    "BIDIRECTIONAL = 1  # 양방향 특성 추출: True(2), False(1)\n",
    "\n",
    "BATCH_SIZE = x.size(0)\n",
    "SEQ_LENGTH = x.size(1)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"SEQ_LENGTH: \", SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80244f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE, batch_first=True, device=device\n",
    ")\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial weights 초기화\n",
    "h_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)\n",
    "c_0 = torch.zeros(NUM_LAYERS * BIDIRECTIONAL, SEQ_LENGTH, HIDDEN_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd517763",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0d4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 레이어 Output, 초기화 (hidden_state, cell_state)\n",
    "lstm_out, (hidden, cell) = lstm(embedding_out)\n",
    "\n",
    "# (batch_size, seq_length, hidden_size)\n",
    "lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c484ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_layers * bidirectional, batch_size, hidden_size)\n",
    "# (num_layers * bidirectional, batch_size, hidden_size)\n",
    "hidden.shape, cell.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4782b8",
   "metadata": {},
   "source": [
    "## Embedding -> LSTM 의 입출력 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingLSTM(\n",
    "    x, vocab_size, embedding_dim, hidden_size, bidirectional, num_layers, device\n",
    "):\n",
    "    \"\"\"\n",
    "    x             : 데이터 입력 (batch_size, seq_length)\n",
    "    vocab_size    : 단어사전의 개수\n",
    "    embedding_dim : 임베딩 차원\n",
    "    hidden_size   : 특성추출의 개수(hyper-parameter)\n",
    "    bidirectional : 양방향 특성 추출: 양방향(True), 단방향(False)\n",
    "    num_layers    : Stacking LSTM 레이어 수, 기본: 1\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    print(f\"===== Part1. 입력(x) =====\\n\")\n",
    "    print(f\"입력(x)의 차원(batch_size({batch_size}), seq_length({x.size(1)}))\")\n",
    "    print(f\"{x.shape}\\n\")\n",
    "\n",
    "    embedding = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
    "    embedding_out = embedding(x)\n",
    "    print(f\"===== Part2. Embedding =====\\n\")\n",
    "    print(\n",
    "        f\"(batch_size({batch_size}), seq_length({x.size(1)}), embedding_dim({embedding_dim}))\"\n",
    "    )\n",
    "    print(f\"{embedding_out.shape}\")\n",
    "\n",
    "    lstm = nn.LSTM(\n",
    "        input_size=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        bidirectional=bidirectional,\n",
    "        batch_first=True,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    bidi = 2 if bidirectional else 1\n",
    "\n",
    "    out, (h, c) = lstm(embedding_out)\n",
    "    print()\n",
    "    print(f\"===== Part3. LSTM =====\\n\")\n",
    "    print(\"out, (h, c) = lstm(x)\\n\")\n",
    "    print(\"LSTM output\")\n",
    "    print(\n",
    "        f\"(batch_size({x.size(0)}), seq_length({x.size(1)}), hidden_size({hidden_size})*bidirectional({bidi}))\"\n",
    "    )\n",
    "    print(f\"{out.shape}\\n\")\n",
    "    print(\"===\" * 8)\n",
    "    print(\"\\n(hidden, cell) state\\n\")\n",
    "    print(\n",
    "        f\"(num_layers({num_layers})*bidirectional({bidi}), batch_size({batch_size}), hidden_size({hidden_size}))\"\n",
    "    )\n",
    "    print(f\"{h.shape}\\n\")\n",
    "    print(\"===\" * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbeddingLSTM(\n",
    "    x,\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=30,\n",
    "    hidden_size=64,\n",
    "    bidirectional=False,\n",
    "    num_layers=2,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7219c",
   "metadata": {},
   "source": [
    "## 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        bidirectional=True,\n",
    "        drop_prob=0.1,\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * self.bidirectional, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden_and_cell_state(self, batch_size, device):\n",
    "        # LSTM 입력시 초기 Cell 에 대한 가중치 초기화를 진행합니다.\n",
    "        # (num_layers*bidirectional, batch_size, hidden_size)\n",
    "        self.hidden_and_cell = (\n",
    "            torch.zeros(\n",
    "                self.num_layers * self.bidirectional, batch_size, self.hidden_size\n",
    "            ).to(device),\n",
    "            torch.zeros(\n",
    "                self.num_layers * self.bidirectional, batch_size, self.hidden_size\n",
    "            ).to(device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (h, c) = self.lstm(x, self.hidden_and_cell)\n",
    "        # (batch_size, seq_length, hidden_size*bidirectional)\n",
    "        # last sequence 의 (batch_size, hidden_size*bidirectional)\n",
    "        h = output[:, -1, :]\n",
    "        o = self.dropout(h)\n",
    "        o = self.relu(self.fc(o))\n",
    "        o = self.dropout(o)\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_classes\": 2,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"embedding_dim\": 16,\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": True,\n",
    "}\n",
    "\n",
    "model = TextClassificationModel(**config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61947b",
   "metadata": {},
   "source": [
    "## 손실 함수 및 옵티마이저 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030efd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 정의: CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fit(model, data_loader, loss_fn, optimizer, device, phase=\"train\"):\n",
    "    if phase == \"train\":\n",
    "        # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "        model.train()\n",
    "    else:\n",
    "        # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다.\n",
    "        model.eval()\n",
    "\n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "\n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(\n",
    "        data_loader, leave=False, unit=\"batch\", total=len(data_loader), mininterval=1\n",
    "    )\n",
    "\n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for txt, lbl in prograss_bar:\n",
    "        # image, label 데이터를 device에 올립니다.\n",
    "        txt, lbl = txt.to(device), lbl.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "            model.init_hidden_and_cell_state(len(txt), device)\n",
    "            # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "            output = model(txt)\n",
    "\n",
    "            # 손실함수에 output, label 값을 대입하여 손실을 계산합니다.\n",
    "            loss = loss_fn(output, lbl)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "                loss.backward()\n",
    "\n",
    "                # 계산된 Gradient를 업데이트 합니다.\n",
    "                optimizer.step()\n",
    "\n",
    "        # output 의 뉴런별 확률 값을 sparse vector 로 변환합니다.\n",
    "        pred = output.argmax(axis=1)\n",
    "\n",
    "        # 정답 개수를 카운트 합니다.\n",
    "        corr += (lbl == pred).sum().item()\n",
    "\n",
    "        # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 누적된 정답수를 전체 개수로 나누어 주면 정확도가 산출됩니다.\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "\n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de61bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 최대 Epoch을 지정합니다.\n",
    "num_epochs = 5\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "STATE_DICT_PATH = \"LSTM-Text-Classification.pth\"\n",
    "\n",
    "# Epoch 별 훈련 및 검증을 수행합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    # Model Training\n",
    "    # 훈련 손실과 정확도를 반환 받습니다.\n",
    "    start = time.time()\n",
    "    train_loss, train_acc = fit(\n",
    "        model, train_loader, loss_fn, optimizer, device, phase=\"train\"\n",
    "    )\n",
    "\n",
    "    # 검증 손실과 검증 정확도를 반환 받습니다.\n",
    "    val_loss, val_acc = fit(\n",
    "        model, validation_loader, loss_fn, optimizer, device, phase=\"eval\"\n",
    "    )\n",
    "\n",
    "    # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "    if val_loss < min_loss:\n",
    "        print(\n",
    "            f\"[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!\"\n",
    "        )\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), STATE_DICT_PATH)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    # Epoch 별 결과를 출력합니다.\n",
    "    print(\n",
    "        f\"[Epoch{epoch+1:02d}] time: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s \\t loss: {train_loss:.5f}, acc: {train_acc:.5f} | val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922511c",
   "metadata": {},
   "source": [
    "## 저장한 가중치 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 저장한 가중치를 로드합니다.\n",
    "model.load_state_dict(torch.load(STATE_DICT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803ce5b",
   "metadata": {},
   "source": [
    "## 최종 검증성능 측정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 검증 손실(validation loss)와 검증 정확도(validation accuracy)를 산출합니다.\n",
    "final_loss, final_acc = fit(\n",
    "    model, validation_loader, loss_fn, optimizer, device, phase=\"eval\"\n",
    ")\n",
    "print(f\"\\nevaluation loss: {final_loss:.5f}, evaluation accuracy: {final_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b63be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
