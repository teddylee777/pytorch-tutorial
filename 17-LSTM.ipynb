{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad9aee",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "- `input_size`: input `x`에 대한 features의 수\n",
    "- `hidden_size`: hidden state의 features의 수\n",
    "- `num_layers`: LSTM을 스택킹(stacking)하는 수\n",
    "- `batch_first`: 입출력 텐서의 형태가 다음과 같음. 기본값은 `False`\n",
    "  - `True`로 설정시 `(batch, seq, feature)`\n",
    "  - `False`로 설정시 `(seq, batch, feature)`\n",
    "  - (주의사항) `hidden_state`, `cell_state`에는 영향을 미치지 않습니다.\n",
    "- `bidirectional`: 양방향 LSTM 구현 여부. 기본값은 `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=10, hidden_size=20, num_layers=1, bidirectional=False, batch_first=True\n",
    ")\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176750e",
   "metadata": {},
   "source": [
    "## 출력에 대한 이해\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db448a3",
   "metadata": {},
   "source": [
    "> 입력에 활용할 텐서 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87057e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력\n",
    "inputs = torch.zeros(1, 35, 10)\n",
    "\n",
    "# LSTM\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10, hidden_size=20, num_layers=2, bidirectional=True, batch_first=True\n",
    ")\n",
    "\n",
    "# 출력\n",
    "outputs, (hidden_state, cell_state) = lstm(inputs)\n",
    "print(outputs.shape, hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.zeros(1, 35, 10)\n",
    "print(inputs.shape)\n",
    "# batch_size, sequence_length, number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2764b9f",
   "metadata": {},
   "source": [
    "`lstm`에 방금 생성한 `inputs`를 입력으로 넣습니다.\n",
    "\n",
    "여기서, `lstm`을 생성할 때 `batch_first`를 `True`로 설정했기 때문에 입력 텐서의 첫 번째 shape는 `batch_size`가 위치해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (hidden_state, cell_state) = lstm(inputs)\n",
    "print(outputs.shape, hidden_state.shape, cell_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3e433",
   "metadata": {},
   "source": [
    "1. `outputs` 의 shape에 대한 이해\n",
    "\n",
    "- `[1, 35, 20]` 형태를 가집니다. 순서대로 `(batch_size, sequence_length, hidden_size(20) x bidirectional(1))` 입니다.\n",
    "\n",
    "2. `hidden_state` 의 shape에 대한 이해\n",
    "\n",
    "- `hidden_state`는 short term memory에 대한 state를 나타냅니다.\n",
    "- `[1, 1, 20]` 형태를 가집니다. 순서대로 `(Bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n",
    "\n",
    "3. `cell_state` 의 shape에 대한 이해\n",
    "\n",
    "- `cell_state`는 long term memory에 대한 state를 나타냅니다.\n",
    "- `[1, 1, 20]` 형태를 가집니다. 순서대로 `(Bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc0d1b",
   "metadata": {},
   "source": [
    "## Stacking LSTM\n",
    "\n",
    "- LSTM를 겹층으로 쌓아올리는 것을 Stacking이라고 합니다. 이렇게 쌓아올렸을 때는 결과 shape가 달라지게 됩니다.\n",
    "- `num_layers`에 쌓고자 하는 층의 개수를 입력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers를 2로 설정\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10, hidden_size=20, num_layers=2, bidirectional=False, batch_first=True\n",
    ")\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0ffb0",
   "metadata": {},
   "source": [
    "> 이전과 동일한 입력 텐서 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.zeros(1, 35, 10)\n",
    "print(inputs.shape)\n",
    "# batch_size, sequence_length, number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (hidden_state, cell_state) = lstm(inputs)\n",
    "print(outputs.shape, hidden_state.shape, cell_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae859c0",
   "metadata": {},
   "source": [
    "1. `outputs` 의 shape에 대한 이해\n",
    "\n",
    "- `[1, 35, 20]` 형태를 가집니다. 순서대로 `(batch_size, sequence_length, hidden_size(20) x bidirectional(1))` 입니다.\n",
    "\n",
    "2. `hidden_state` 의 shape에 대한 이해\n",
    "\n",
    "- `hidden_state`는 short term memory에 대한 state를 나타냅니다.\n",
    "- `[2, 1, 20]` 형태를 가집니다. 순서대로 `(Bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n",
    "- 출력 텐서의 첫 번째 shape은 `Bidirectional(1) x number of layers(2)=2` 결과입니다.\n",
    "\n",
    "3. `cell_state` 의 shape에 대한 이해\n",
    "\n",
    "- `cell_state`는 long term memory에 대한 state를 나타냅니다.\n",
    "- `[2, 1, 20]` 형태를 가집니다. 순서대로 `(Bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n",
    "- 출력 텐서의 첫 번째 shape은 `Bidirectional(1) x number of layers(2)=2` 결과입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffd84b",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n",
    "- 양방향 LSTM을 사용하기 위해서는 `bidirectional=True`로 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320aa7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional=True 설정\n",
    "lstm = nn.LSTM(\n",
    "    input_size=10, hidden_size=20, num_layers=2, bidirectional=True, batch_first=True\n",
    ")\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba26009",
   "metadata": {},
   "source": [
    "> 이전과 동일한 입력 텐서 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.zeros(1, 35, 10)\n",
    "print(inputs.shape)\n",
    "# batch_size, sequence_length, number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733116b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (hidden_state, cell_state) = lstm(inputs)\n",
    "print(outputs.shape, hidden_state.shape, cell_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df28337",
   "metadata": {},
   "source": [
    "1. `outputs` 의 shape에 대한 이해\n",
    "\n",
    "- `[1, 35, 40]` 형태를 가집니다. 순서대로 `(batch_size, sequence_length, hidden_size(20) x bidirectional(2))` 입니다.\n",
    "- 출력 텐서의 마지막 shape은 `hidden_size(20) x bidirectional(2)=40` 결과입니다.\n",
    "\n",
    "2. `hidden_state` 의 shape에 대한 이해\n",
    "\n",
    "- `hidden_state`는 short term memory에 대한 state를 나타냅니다.\n",
    "- `[4, 1, 20]` 형태를 가집니다. 순서대로 `(bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n",
    "- 출력 텐서의 첫 번째 shape은 `bidirectional(2) x number of layers(2)=4` 결과입니다.\n",
    "\n",
    "3. `cell_state` 의 shape에 대한 이해\n",
    "\n",
    "- `cell_state`는 long term memory에 대한 state를 나타냅니다.\n",
    "- `[4, 1, 20]` 형태를 가집니다. 순서대로 `(bidirectional x number of layers, batch_size, hidden_size)` 입니다.\n",
    "- 출력 텐서의 첫 번째 shape은 `bidirectional(2) x number of layers(2)=4` 결과입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb676e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
