{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data_dir = \"data\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, \"kor_eng.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean = df[\"korean\"]\n",
    "english = df[\"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e1f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "english[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ce925",
   "metadata": {},
   "source": [
    "## 한글 정규화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdebcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 한글, 영어, 숫자, 공백, ?!.,을 제외한 나머지 문자 제거\n",
    "korean_pattern = r\"[^ ?,.!A-Za-z0-9가-힣+]\"\n",
    "\n",
    "# 패턴 컴파일\n",
    "normalizer = re.compile(korean_pattern)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"수정 전: {korean[10]}\")\n",
    "print(f'수정 후: {normalizer.sub(\"\", korean[10])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c918464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"수정 전: {english[10]}\")\n",
    "print(f'수정 후: {normalizer.sub(\"\", english[10])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sentence):\n",
    "    return normalizer.sub(\"\", sentence)\n",
    "\n",
    "\n",
    "normalize(korean[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d1d77",
   "metadata": {},
   "source": [
    "## 한글 형태소 분석기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003483bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab, Okt\n",
    "\n",
    "# 형태소 분석기\n",
    "mecab = Mecab()\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab\n",
    "mecab.morphs(normalize(korean[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okt\n",
    "okt.morphs(normalize(korean[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence, tagger, korean=True):\n",
    "    sentence = normalize(sentence)\n",
    "    if korean:\n",
    "        sentence = tagger.morphs(sentence)\n",
    "        sentence = \" \".join(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글\n",
    "clean_text(korean[10], okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea788a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어\n",
    "clean_text(english[10], None, korean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(korean), len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce95632",
   "metadata": {},
   "outputs": [],
   "source": [
    "koreans = [clean_text(sent, okt, korean=True) for sent in korean.values[:1000]]\n",
    "englishes = [clean_text(sent, None, korean=False)\n",
    "             for sent in english.values[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bfa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "koreans[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73844c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "englishes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa240d8",
   "metadata": {},
   "source": [
    "## 단어 사전 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "\n",
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        self.word2index = {\n",
    "            '<PAD>': PAD_TOKEN,\n",
    "            '<SOS>': SOS_TOKEN,\n",
    "            '<EOS>': EOS_TOKEN,\n",
    "        }\n",
    "        self.word2count = {}\n",
    "        self.index2word = {\n",
    "            PAD_TOKEN: '<PAD>',\n",
    "            SOS_TOKEN: '<SOS>',\n",
    "            EOS_TOKEN: '<EOS>'\n",
    "        }\n",
    "\n",
    "        self.n_words = 3  # PAD, SOS, EOS 포함\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60766793",
   "metadata": {},
   "outputs": [],
   "source": [
    "koreans[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa82bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"원문: {koreans[10]}\")\n",
    "lang = WordVocab()\n",
    "lang.add_sentence(koreans[10])\n",
    "print(\"===\" * 10)\n",
    "print(\"단어사전\")\n",
    "print(lang.word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb512c",
   "metadata": {},
   "source": [
    "## Padding to sequences 에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "sentence_length = 6\n",
    "\n",
    "sentence_tokens = np.random.randint(low=3, high=100, size=(sentence_length,))\n",
    "sentence_tokens = sentence_tokens.tolist()\n",
    "print(f\"Generated Sentence: {sentence_tokens}\")\n",
    "\n",
    "sentence_tokens = sentence_tokens[: (max_length - 1)]\n",
    "\n",
    "token_length = len(sentence_tokens)\n",
    "\n",
    "# append <EOS> Token\n",
    "sentence_tokens.append(2)\n",
    "\n",
    "for i in range(token_length, max_length - 1):\n",
    "    # add <PAD> Token\n",
    "    sentence_tokens.append(0)\n",
    "\n",
    "print(f\"Output: {sentence_tokens}\")\n",
    "print(f\"Total Length: {len(sentence_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd2384",
   "metadata": {},
   "source": [
    "## 전처리 모듈 클래스화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(idx)\n",
    "    print(row[\"korean\"])\n",
    "    print(row[\"english\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab, Okt\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path, max_length=32):\n",
    "        super(TextDataset, self).__init__()\n",
    "        data_dir = 'data'\n",
    "\n",
    "        self.PAD_TOKEN = 0\n",
    "        self.SOS_TOKEN = 1\n",
    "        self.EOS_TOKEN = 2\n",
    "\n",
    "        MIN_LENGTH = 5\n",
    "\n",
    "        tagger = Mecab()\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # CSV 데이터 로드\n",
    "        df = pd.read_csv(os.path.join(data_dir, csv_path))\n",
    "\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        normalizer = re.compile(korean_pattern)\n",
    "\n",
    "        koreans_clean = []\n",
    "        englishes_clean = []\n",
    "\n",
    "        koreans_wordvocab = WordVocab()\n",
    "        englishes_wordvocab = WordVocab()\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            src = row['korean']\n",
    "            tgt = row['english']\n",
    "\n",
    "            src = clean_text(src, tagger, korean=True)\n",
    "            tgt = clean_text(tgt, None, korean=False)\n",
    "\n",
    "            if len(src.split()) > MIN_LENGTH and len(tgt.split()) > MIN_LENGTH:\n",
    "                koreans_wordvocab.add_sentence(src)\n",
    "                koreans_clean.append(src)\n",
    "                englishes_wordvocab.add_sentence(tgt)\n",
    "                englishes_clean.append(tgt)\n",
    "\n",
    "        self.koreans = koreans_clean\n",
    "        self.englishes = englishes_clean\n",
    "        self.koreans_wordvocab = koreans_wordvocab\n",
    "        self.englishes_wordvocab = englishes_wordvocab\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(sentence):\n",
    "        return normalizer.sub(\"\", sentence)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(sentence, tagger, korean=True):\n",
    "        sentence = normalize(sentence)\n",
    "        if korean:\n",
    "            sentence = tagger.morphs(sentence)\n",
    "            sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "\n",
    "    def texts_to_sequences(self, sentence, korean=True):\n",
    "        if korean:\n",
    "            return [self.koreans_wordvocab.word2index[w] for w in sentence.split()]\n",
    "        else:\n",
    "            return [self.englishes_wordvocab.word2index[w] for w in sentence.split()]\n",
    "\n",
    "    def pad_sequence(self, sentence_tokens):\n",
    "        sentence_tokens = sentence_tokens[:(self.max_length-1)]\n",
    "        token_length = len(sentence_tokens)\n",
    "\n",
    "        # append <EOS> Token\n",
    "        sentence_tokens.append(self.EOS_TOKEN)\n",
    "\n",
    "        for i in range(token_length, (self.max_length-1)):\n",
    "            # add <PAD> Token\n",
    "            sentence_tokens.append(self.PAD_TOKEN)\n",
    "        return sentence_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.koreans[idx]\n",
    "        inputs_sequences = self.texts_to_sequences(inputs, korean=True)\n",
    "        inputs_padded = self.pad_sequence(inputs_sequences)\n",
    "\n",
    "        outputs = self.englishes[idx]\n",
    "        outputs_sequences = self.texts_to_sequences(outputs, korean=False)\n",
    "        outputs_padded = self.pad_sequence(outputs_sequences)\n",
    "\n",
    "        return torch.tensor(inputs_padded), torch.tensor(outputs_padded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.koreans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02edbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 32\n",
    "\n",
    "dataset = TextDataset(\"kor_eng.csv\", max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3369f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b910123",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515963f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_sentence(x.numpy(), korean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a70f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_sentence(y.numpy(), korean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ed00f",
   "metadata": {},
   "source": [
    "## train / test 데이터셋 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(dataset) - train_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec552a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001bf5a",
   "metadata": {},
   "source": [
    "## DataLoader 생성\n",
    "\n",
    "- 배치 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f6490",
   "metadata": {},
   "source": [
    "## Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70706f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                          hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=False,\n",
    "                          batch_first=True,\n",
    "                          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b68660",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9b4a0",
   "metadata": {},
   "source": [
    "### Embedding Layer의 입/출력 shape에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e645c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20  # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.koreans_wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x)\n",
    "print(x.shape)\n",
    "print(embedded.shape)\n",
    "# input:  (batch_size, sequence_length)\n",
    "# output: (batch_size, sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0f2cf",
   "metadata": {},
   "source": [
    "### GRU Layer의 입/출력 shape에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20  # 임베딩 차원\n",
    "hidden_size = 32\n",
    "\n",
    "gru = nn.GRU(embedding_dim,\n",
    "             hidden_size,\n",
    "             num_layers=1,\n",
    "             bidirectional=False,\n",
    "             batch_first=True)\n",
    "\n",
    "o, h = gru(embedded)\n",
    "\n",
    "print(o.shape)\n",
    "# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e3cee",
   "metadata": {},
   "source": [
    "### Encoder의 입/출력 shape에 대한 이해\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VOCABS = dataset.koreans_wordvocab.n_words\n",
    "print(f\"number of vocabs: {NUM_VOCABS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d30d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 정의\n",
    "encoder = Encoder(dataset.koreans_wordvocab.n_words,\n",
    "                  hidden_size=32,\n",
    "                  embedding_dim=20,\n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18357f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder에 x 통과 후 output, hidden_size 의 shape 확인\n",
    "o, h = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f66213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.shape)\n",
    "# output      : (batch_size, sequence_length, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786ad2b",
   "metadata": {},
   "source": [
    "## Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                          hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          bidirectional=False,\n",
    "                          )\n",
    "        self.fc = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        # (1, batch_size, sequence_length, hidden_size) 로 변환\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = F.relu(self.embedding(x))\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        output = self.fc(output.squeeze(0))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.abs(torch.randn(size=(1, 16)).long())\n",
    "print(x)\n",
    "x.shape\n",
    "# batch_size = 16 이라 가정했을 때,\n",
    "# (1, batch_size)\n",
    "# 여기서 batch_size => (1, batch_size) 로 shape 변환을 선행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20  # 임베딩 차원\n",
    "embedding = nn.Embedding(dataset.koreans_wordvocab.n_words, embedding_dim)\n",
    "\n",
    "embedded = embedding(x.long())\n",
    "embedded.shape\n",
    "# (1, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20  # 임베딩 차원\n",
    "hidden_size = 32\n",
    "\n",
    "gru = nn.GRU(embedding_dim,\n",
    "             hidden_size,\n",
    "             num_layers=1,\n",
    "             bidirectional=False,\n",
    "             batch_first=False,  # batch_first=False로 지정\n",
    "             )\n",
    "\n",
    "o, h = gru(embedded)\n",
    "print(o.shape)\n",
    "# output shape: (sequence_length, batch_size, hidden_size(32) x bidirectional(1))\n",
    "print(h.shape)\n",
    "# hidden_state shape: (Bidirectional(1) x number of layers(1), batch_size, hidden_size(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(32, 1024)  # output dimension을 1024개로 가정\n",
    "\n",
    "print(o[0].shape)\n",
    "# input : (batch_size, output from GRU)\n",
    "output = fc(o[0])\n",
    "# output: (batch_size, output dimension)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(num_vocabs=dataset.englishes_wordvocab.n_words,\n",
    "                  hidden_size=32,\n",
    "                  embedding_dim=20,\n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "o, h = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape, h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6789c1",
   "metadata": {},
   "source": [
    "인코더(Encoder)로부터 생성된 hidden_state(h)와 SOS 토큰을 디코더(Decoder)의 입력으로 넣어줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.abs(torch.randn(size=(16,)).long())\n",
    "print(x)\n",
    "x.shape\n",
    "# batch_size = 16 이라 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723aa6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden = decoder(x, h)\n",
    "decoder_output.shape, decoder_hidden.shape\n",
    "# (batch_size, num_vocabs), (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a486dd",
   "metadata": {},
   "source": [
    "- `decoder_output`은 `(batch_size, num_vocabs) shape로 출력\n",
    "- `decoder_hidden`의 shape는 입력으로 넣어준 shape와 동일함을 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd459e0",
   "metadata": {},
   "source": [
    "## Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0adcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8727da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, teacher_forcing_ratio=0.5):\n",
    "        # inputs : (batch_size, sequence_length)\n",
    "        # outputs: (batch_size, sequence_length)\n",
    "\n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_num_vocabs = self.decoder.num_vocabs\n",
    "\n",
    "        # 리턴할 예측된 outputs를 저장할 임시 변수\n",
    "        predicted_outputs = torch.zeros(\n",
    "            output_length, batch_size, output_num_vocabs).to(self.device)\n",
    "\n",
    "        _, decoder_hidden = self.encoder(inputs)\n",
    "\n",
    "        decoder_input = torch.full(\n",
    "            (batch_size,), SOS_TOKEN, device=self.device)\n",
    "\n",
    "        for t in range(1, output_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "\n",
    "            # t번째 단어에 디코더의 output 저장\n",
    "            predicted_outputs[t] = decoder_output\n",
    "\n",
    "            # teacher forcing 적용 여부 확률로 결정\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # top1 예측\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            # teacher forcing 인 경우 ground truth 값을\n",
    "            # 그렇지 않은 경우, 예측 값을 다음 input으로 지정\n",
    "            decoder_input = outputs[:, t] if teacher_force else top1\n",
    "\n",
    "        # (batch_size, sequence_length, num_vocabs)\n",
    "        return predicted_outputs.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ccc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038673be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=dataset.koreans_wordvocab.n_words,\n",
    "                  hidden_size=32,\n",
    "                  embedding_dim=20,\n",
    "                  num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=dataset.englishes_wordvocab.n_words,\n",
    "                  hidden_size=32,\n",
    "                  embedding_dim=20,\n",
    "                  num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = seq2seq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012cd56",
   "metadata": {},
   "source": [
    "## 훈련\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be02adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_NUM_VOCABS = dataset.koreans_wordvocab.n_words\n",
    "OUTPUT_NUM_VOCABS = dataset.englishes_wordvocab.n_words\n",
    "HIDDEN_SIZE = 512\n",
    "EMBEDDIMG_DIM = 256\n",
    "\n",
    "print(\n",
    "    f'input_num_vocabs: {INPUT_NUM_VOCABS}, output_num_vocabs: {OUTPUT_NUM_VOCABS}')\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(num_vocabs=INPUT_NUM_VOCABS,\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  embedding_dim=EMBEDDIMG_DIM,\n",
    "                  num_layers=1)\n",
    "# Decoder 정의\n",
    "decoder = Decoder(num_vocabs=OUTPUT_NUM_VOCABS,\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  embedding_dim=EMBEDDIMG_DIM,\n",
    "                  num_layers=1)\n",
    "\n",
    "# Seq2Seq 생성\n",
    "model = Seq2Seq(encoder.to(device), decoder.to(device), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a959cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b38a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, loss_fn, device):\n",
    "\n",
    "    running_loss = 0\n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x, y)\n",
    "        output_dim = output.size(2)\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        y = y[1:].view(-1)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db078cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 200\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_loader))\n",
    "model.eval()\n",
    "x, y = x.to(device), y.to(device)\n",
    "prediction = model(x, y, teacher_forcing_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction.squeeze(0).argmax(1)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.detach().cpu().numpy()[0]\n",
    "y = y.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_sentence(sequences, korean):\n",
    "    outputs = []\n",
    "    for p in sequences:\n",
    "        if korean:\n",
    "            word = dataset.koreans_wordvocab.index2word[p]\n",
    "        else:\n",
    "            word = dataset.englishes_wordvocab.index2word[p]\n",
    "\n",
    "        if p not in [SOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
    "            outputs.append(word)\n",
    "        if word == EOS_TOKEN:\n",
    "            break\n",
    "    return ' '.join(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_sentence(x, korean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_sentence(preds, korean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_sentence(y, korean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2674fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
