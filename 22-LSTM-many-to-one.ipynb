{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dba8f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 1,000 단위 표기\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# Unicode warning 제거 (폰트 관련 경고메시지)\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "# 그래프 출력 사이즈 설정\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data 경로 설정\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# 시드설정\n",
    "SEED = 123\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc7a5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-17</th>\n",
       "      <td>465.864</td>\n",
       "      <td>468.174</td>\n",
       "      <td>452.422</td>\n",
       "      <td>457.334</td>\n",
       "      <td>457.334</td>\n",
       "      <td>21056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-18</th>\n",
       "      <td>456.860</td>\n",
       "      <td>456.860</td>\n",
       "      <td>413.104</td>\n",
       "      <td>424.440</td>\n",
       "      <td>424.440</td>\n",
       "      <td>34483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-19</th>\n",
       "      <td>424.103</td>\n",
       "      <td>427.835</td>\n",
       "      <td>384.532</td>\n",
       "      <td>394.796</td>\n",
       "      <td>394.796</td>\n",
       "      <td>37919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-20</th>\n",
       "      <td>394.673</td>\n",
       "      <td>423.296</td>\n",
       "      <td>389.883</td>\n",
       "      <td>408.904</td>\n",
       "      <td>408.904</td>\n",
       "      <td>36863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-21</th>\n",
       "      <td>408.085</td>\n",
       "      <td>412.426</td>\n",
       "      <td>393.181</td>\n",
       "      <td>398.821</td>\n",
       "      <td>398.821</td>\n",
       "      <td>26580100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close  Adj Close    Volume\n",
       "Date                                                           \n",
       "2014-09-17 465.864 468.174 452.422 457.334    457.334  21056800\n",
       "2014-09-18 456.860 456.860 413.104 424.440    424.440  34483200\n",
       "2014-09-19 424.103 427.835 384.532 394.796    394.796  37919700\n",
       "2014-09-20 394.673 423.296 389.883 408.904    408.904  36863600\n",
       "2014-09-21 408.085 412.426 393.181 398.821    398.821  26580100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FinanceDataReader as fdr\n",
    "\n",
    "df = fdr.DataReader('BTC/USD')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87fc0120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2014-09-17      457.334\n",
       "2014-09-18      424.440\n",
       "2014-09-19      394.796\n",
       "2014-09-20      408.904\n",
       "2014-09-21      398.821\n",
       "                ...    \n",
       "2023-06-29   30,445.352\n",
       "2023-06-30   30,477.252\n",
       "2023-07-01   30,590.078\n",
       "2023-07-02   30,620.770\n",
       "2023-07-03   30,747.822\n",
       "Name: Close, Length: 3212, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb3f59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00414359],\n",
       "       [0.00365546],\n",
       "       [0.00321557],\n",
       "       [0.00342492],\n",
       "       [0.0032753 ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "series = scaler.fit_transform(df['Close'].values.reshape(-1, 1))\n",
    "series[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026942c",
   "metadata": {},
   "source": [
    "## Windowed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "26b6751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PREDICTIONS = 1\n",
    "WINDOW_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "454765f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(series, window_size=WINDOW_SIZE, n_predictions=N_PREDICTIONS):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    for i in range(len(series) - window_size - n_predictions +1):\n",
    "        Xs.append(series[i:i+window_size])\n",
    "        Ys.append(series[i+window_size: i+window_size+n_predictions])\n",
    "    return np.array(Xs), np.array(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4f2c827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "Xs, Ys = make_dataset(series.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1f04d96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3182, 30, 1), (3182, 1))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = np.expand_dims(Xs, -1)\n",
    "Xs.shape, Ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9bfee246",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 50\n",
    "\n",
    "x_train, y_train = Xs[:-n_splits], Ys[:-n_splits]\n",
    "x_valid, y_valid = Xs[-n_splits:], Ys[-n_splits:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "939dd163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3132, 30, 1), (3132, 1))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7077fcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 30, 1), (50, 1))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a9df3ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2023-06-28   30,086.246\n",
       "2023-06-29   30,445.352\n",
       "2023-06-30   30,477.252\n",
       "2023-07-01   30,590.078\n",
       "2023-07-02   30,620.770\n",
       "2023-07-03   30,747.822\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Close'].tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "53bd40a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39913243],\n",
       "       [0.39978444],\n",
       "       [0.37961832],\n",
       "       [0.40156095],\n",
       "       [0.38831266],\n",
       "       [0.39071987],\n",
       "       [0.39030672],\n",
       "       [0.38097081],\n",
       "       [0.38229043],\n",
       "       [0.38173147],\n",
       "       [0.38197229],\n",
       "       [0.37018912],\n",
       "       [0.3768923 ],\n",
       "       [0.38803761],\n",
       "       [0.39075636],\n",
       "       [0.38816745],\n",
       "       [0.39580696],\n",
       "       [0.41771654],\n",
       "       [0.44294047],\n",
       "       [0.44123373],\n",
       "       [0.45285566],\n",
       "       [0.45067765],\n",
       "       [0.44966215],\n",
       "       [0.44655879],\n",
       "       [0.45274727],\n",
       "       [0.44381524],\n",
       "       [0.4491441 ],\n",
       "       [0.44961748],\n",
       "       [0.45129174],\n",
       "       [0.45174718]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "72e299f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45363255])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60b727",
   "metadata": {},
   "source": [
    "## Tensor DataSet 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b12927f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(x, device):\n",
    "    return torch.FloatTensor(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "911486d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device 설정 (cuda 혹은 cpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5e9ce14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = make_tensor(x_train, device=device)\n",
    "y_train = make_tensor(y_train, device=device)\n",
    "x_valid = make_tensor(x_valid, device=device)\n",
    "y_valid = make_tensor(y_valid, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d59bbac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid_ds = torch.utils.data.TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8655a",
   "metadata": {},
   "source": [
    "## DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "11ae456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_ds, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ed8ab",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a5321741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "acb97a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, bidirectional=True):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "        \n",
    "    def init_hidden_and_cell_state(self, batch_size, device):\n",
    "        self.hidden_and_cell = (\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "            torch.zeros(self.num_layers*self.bidirectional, batch_size, self.hidden_size).to(device),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, (h, c) = self.lstm(x, self.hidden_and_cell)\n",
    "        h = output[:, -1, :]\n",
    "        o = self.relu(self.fc(h))\n",
    "        o = self.out(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "38f0d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 1,\n",
    "    'hidden_size': 32, \n",
    "    'num_layers': 2, \n",
    "    'bidirectional': False,\n",
    "    'output_size': N_PREDICTIONS, \n",
    "    \n",
    "}\n",
    "\n",
    "model = BaseModel(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "783e1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.to(device), y.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.init_hidden_and_cell_state(batch_size, device)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5c3a6b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97c274",
   "metadata": {},
   "source": [
    "## 손실함수 & 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83469512",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_fn = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0c9f45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecfd2ce",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "eec045e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "    model.train()\n",
    "    \n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_loss = 0\n",
    "    \n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for idx, (xs, ys) in enumerate(prograss_bar):\n",
    "        # txt, lbl 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "        \n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # LSTM Weight 초기화\n",
    "        model.init_hidden_and_cell_state(len(xs), device)\n",
    "        \n",
    "        # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "        output = model(xs)\n",
    "        \n",
    "        # 손실함수에 output, lbl 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, ys)\n",
    "        \n",
    "        # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch 별 loss 계산하여 누적합을 구합니다.\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 프로그레스바에 학습 상황 업데이트\n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}\")\n",
    "        \n",
    "    \n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss\n",
    "    return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "90c2bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다. \n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "    \n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for xs, ys in data_loader:\n",
    "            # txt, lbl 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "            xs = xs.to(device)\n",
    "            ys = ys.to(device)\n",
    "            \n",
    "            # LSTM Weight 초기화\n",
    "            model.init_hidden_and_cell_state(len(xs), device)\n",
    "    \n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(xs)\n",
    "            \n",
    "            # 검증 손실을 구합니다.\n",
    "            loss = loss_fn(output, ys)\n",
    "            \n",
    "            # batch 별 loss 계산하여 누적합을 구합니다.\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b9293681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 241.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.00004. Saving Model!\n",
      "epoch 01, loss: 0.00007, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 331.22batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 331.46batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 328.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 330.25batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.68batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 331.18batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 327.60batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 330.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 330.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 330.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.71batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.39batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.53batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 330.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.10batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.39batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.00004 to 0.00004. Saving Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.99batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.55batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.65batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.46batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.50batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.22batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.72batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.72batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.13batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.07batch/s]\n",
      "training loss: 0.00007: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.86batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.72batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.91batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.28batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.13batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.03batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.04batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.87batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.72batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.07batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.76batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.92batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.01batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.93batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.01batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.32batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.86batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.90batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.92batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.04batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.60batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.86batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.85batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.12batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 101, loss: 0.00006, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 326.21batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.86batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.00batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.93batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.28batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.52batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 364.68batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.50batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 346.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 353.19batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.30batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.30batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.55batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.37batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.44batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.38batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 341.49batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 354.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 354.75batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 354.50batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 352.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 367.71batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 369.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.08batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 368.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.22batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.20batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.13batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.21batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 362.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 360.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.20batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 347.36batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 332.16batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.38batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.97batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.08batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.92batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.40batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.29batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.35batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.22batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.38batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.08batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.20batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.38batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.44batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.35batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.32batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.10batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.42batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.20batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.36batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.68batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.76batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.59batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.57batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.07batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.10batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 201, loss: 0.00006, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 346.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 355.49batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 355.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 355.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 348.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.90batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.21batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.57batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.28batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.59batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.52batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.99batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.95batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.39batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.17batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.93batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.08batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.46batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.53batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.57batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.38batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.68batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.22batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.35batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.40batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.29batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.12batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.37batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 331.49batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 325.74batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.55batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.49batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.52batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 325.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.68batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.95batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.88batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.00batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.85batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.04batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.32batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.63batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 325.25batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.28batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.60batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.44batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 301, loss: 0.00006, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.57batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.40batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 324.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 333.75batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.10batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.86batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 332.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 332.39batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 334.64batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 332.52batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 333.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 333.74batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 398.23batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 369.77batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.42batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 341.16batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 362.23batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.04batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.48batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 394.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 364.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 347.43batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 366.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 374.39batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.36batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.90batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.19batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 356.07batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 362.33batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.71batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.32batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.88batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.62batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.45batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.96batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.90batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.28batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.17batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 347.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 355.88batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.96batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.20batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.93batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.41batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.69batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 350.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 350.42batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 351.12batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 349.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 350.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 348.59batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 352.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 344.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.03batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 349.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 348.60batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.60batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.05batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.04batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 370.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.55batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.32batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.59batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 384.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 389.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.88batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.87batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 401, loss: 0.00006, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.17batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.36batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.07batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 347.07batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 404.76batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 405.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 404.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 363.60batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.01batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.76batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.03batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.39batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.52batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.40batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.64batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.71batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 339.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.97batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.66batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.03batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 345.61batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 385.77batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 334.68batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.85batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 335.53batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.50batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.01batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.54batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.56batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.37batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.88batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.42batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.98batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.64batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 391.27batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 336.62batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.12batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.53batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 338.03batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 337.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.46batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.40batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.70batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.23batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.63batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.96batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.05batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.85batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.47batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 325.65batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.14batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.49batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.99batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.08batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.89batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.24batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 327.78batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.51batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.92batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 328.95batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.97batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.84batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.06batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.83batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.74batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.91batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.99batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.02batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.99batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.73batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.16batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.34batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.58batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.80batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.09batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 501, loss: 0.00006, val_loss: 0.00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.94batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.05batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.90batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.96batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.05batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.37batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.11batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.79batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.31batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.81batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 330.15batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.96batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.67batch/s]\n",
      "training loss: 0.00006: 100%|███████████████| 98/98 [00:00<00:00, 329.39batch/s]\n",
      "training loss: 0.00006:  78%|███████████▋   | 76/98 [00:00<00:00, 333.63batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Epoch 별 훈련 및 검증을 수행합니다.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Model Training\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 훈련 손실을 반환 받습니다.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 검증 손실을 반환 받습니다.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m model_evaluate(model, valid_loader, loss_fn, device)   \n",
      "Cell \u001b[0;32mIn[225], line 36\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# batch 별 loss 계산하여 누적합을 구합니다.\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 프로그레스바에 학습 상황 업데이트\u001b[39;00m\n\u001b[1;32m     39\u001b[0m prograss_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 최대 Epoch을 지정합니다.\n",
    "num_epochs = 1000\n",
    "\n",
    "# checkpoint로 저장할 모델의 이름을 정의 합니다.\n",
    "model_name = 'TimeSeries-LSTM'\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "# Epoch 별 훈련 및 검증을 수행합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    # Model Training\n",
    "    # 훈련 손실을 반환 받습니다.\n",
    "    train_loss = model_train(model, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "    # 검증 손실을 반환 받습니다.\n",
    "    val_loss = model_evaluate(model, valid_loader, loss_fn, device)   \n",
    "    \n",
    "    # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{model_name}.pth')\n",
    "    \n",
    "    # Epoch 별 결과를 출력합니다.\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, val_loss: {val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c3381",
   "metadata": {},
   "source": [
    "## 저장한 가중치 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2cf297a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{model_name}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb162667",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- `N_PREDICTION` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "579f7cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 1])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_data = series[-WINDOW_SIZE:]\n",
    "last_data = torch.FloatTensor(last_data).to(device)\n",
    "last_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c818036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.init_hidden_and_cell_state(1, device)\n",
    "output = model(last_data)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "56e82747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3998],\n",
       "         [0.3796],\n",
       "         [0.4016],\n",
       "         [0.3883],\n",
       "         [0.3907],\n",
       "         [0.3903],\n",
       "         [0.3810],\n",
       "         [0.3823],\n",
       "         [0.3817],\n",
       "         [0.3820],\n",
       "         [0.3702],\n",
       "         [0.3769],\n",
       "         [0.3880],\n",
       "         [0.3908],\n",
       "         [0.3882],\n",
       "         [0.3958],\n",
       "         [0.4177],\n",
       "         [0.4429],\n",
       "         [0.4412],\n",
       "         [0.4529],\n",
       "         [0.4507],\n",
       "         [0.4497],\n",
       "         [0.4466],\n",
       "         [0.4527],\n",
       "         [0.4438],\n",
       "         [0.4491],\n",
       "         [0.4496],\n",
       "         [0.4513],\n",
       "         [0.4517],\n",
       "         [0.4536]]], device='cuda:0')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_data = torch.unsqueeze(last_data, 0)#.unsqueeze(0)\n",
    "last_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9d86e684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor(last_data)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "960c8431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEXT_TIMESTEPS = 50\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "inputs = torch.tensor(last_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(last_data)\n",
    "    for i in range(NEXT_TIMESTEPS):\n",
    "        model.init_hidden_and_cell_state(1, device)\n",
    "        output = model(inputs)\n",
    "        results.append(output[0, 0])\n",
    "        inputs = torch.cat([inputs.squeeze()[-(WINDOW_SIZE-1):], output[0, 0].view(1)], axis=0)\n",
    "        inputs = torch.unsqueeze(inputs, -1).unsqueeze(0)\n",
    "        \n",
    "results = torch.tensor(results).numpy()\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1855f050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(series.flatten()[-NEXT_TIMESTEPS:], color='black', linestyle='dotted', label='real')\n",
    "ax.plot(results, color='tomato', label='prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc3c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
